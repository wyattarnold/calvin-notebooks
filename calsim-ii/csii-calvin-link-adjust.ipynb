{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Prepare necessary COSVF default inputs\n",
    "import os,sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from ast import literal_eval\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.core.arrays.categorical import contains\n",
    "\n",
    "# %% Load pyVIN\n",
    "calvin_dir = os.path.abspath('../')\n",
    "if str(calvin_dir)!=sys.path:\n",
    "    sys.path.append(calvin_dir)\n",
    "from calvin import CALVIN, cosvfea\n",
    "from calvin import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_climate = '1995HTD'\n",
    "scenario_name = 'wsip{}csii'.format(scenario_climate)\n",
    "lf_dir = '../my-models/calvin-lf'\n",
    "\n",
    "# %% PF scenarios\n",
    "pf_dir = '../my-models/calvin-pf'\n",
    "\n",
    "\n",
    "# %% calsim flows\n",
    "cs_flows = pd.read_csv('csii-wsip-flows-and-storages.csv')\n",
    "cs_sr_init = pd.read_csv('csii-wsip-init-storages.csv')\n",
    "cs_map = pd.read_csv('csii-calvin-mapping.csv')\n",
    "cs_map['SV'] = cs_map['SV'].apply(literal_eval)\n",
    "cs_map['DV'] = cs_map['DV'].apply(literal_eval)\n",
    "cs_map['S'] = cs_map['S'].apply(literal_eval)\n",
    "#  convert cfs to TAF/month\n",
    "cs_flows['C300'] =  cs_flows['C300']*60.37/1000\n",
    "cs_flows.iloc[:,3:] = cs_flows.iloc[:,3:].round(3)\n",
    "\n",
    "\n",
    "##### LOAD ORIGINAL PF LINKS ####\n",
    "#  %% load perfect foresight links\n",
    "pf_links = pd.read_csv(os.path.join('../my-models/calvin-pf-original', 'links82yr.csv'))\n",
    "# remove debug nodes\n",
    "pf_links = pf_links.loc[\n",
    "    (~pf_links.i.str.contains('DBUG')) & (~pf_links.j.str.contains('DBUG'))]\n",
    "# create a column for edges (w/o dates)\n",
    "pf_links.insert(0, 'edge', \n",
    "    value=pf_links.i.str.split('.').str[0]+'_'+pf_links.j.str.split('.').str[0])\n",
    "pf_links.insert(1,'i_node',\n",
    "    value=pf_links.i.map(lambda x: x.split('.')[0]))\n",
    "pf_links.insert(2,'j_node',\n",
    "    value=pf_links.j.map(lambda x: x.split('.')[0]))\n",
    "pf_links.insert(0, 'year',\n",
    "    value=pd.DatetimeIndex(pf_links.i.str.split('.').str[1]).year)\n",
    "pf_links.insert(1, 'month',\n",
    "    value=pd.DatetimeIndex(pf_links.i.str.split('.').str[1]).month)\n",
    "pf_links.insert(0, 'date',\n",
    "    value=pd.DatetimeIndex(pf_links.i.str.split('.').str[1]))\n",
    "# %%\n",
    "pf_links_mod = pf_links.copy()\n",
    "\n",
    "\n",
    "\n",
    "#### INITIAL STOAGES FOR PF ####\n",
    "# %% set initial storages on PF links\n",
    "for sr in cs_sr_init.sr:\n",
    "    pf_links_mod.loc[pf_links_mod.edge=='INITIAL_{}'.format(sr),['lower_bound','upper_bound']] = cs_sr_init.loc[cs_sr_init.sr==sr,'calsim'].iloc[0]\n",
    "    print(pf_links_mod.loc[pf_links_mod.edge=='INITIAL_{}'.format(sr),['lower_bound','upper_bound']])\n",
    "\n",
    "\n",
    "\n",
    "#### INFLOWS FOR PF ####\n",
    "# %% historical flows\n",
    "pf_flows_compare = pd.DataFrame()\n",
    "for idx,row in cs_map.iterrows():\n",
    "    if row.SV or row.DV:\n",
    "        link = str(row.Inflow_link)\n",
    "        print(link)\n",
    "        flows_original = pf_links.loc[pf_links.edge==link].copy()\n",
    "        print(flows_original.lower_bound.sum())\n",
    "        if row.SV:\n",
    "            flows_csii = cs_flows.loc[cs_flows.scenario==scenario_climate,row.SV].sum(axis=1)\n",
    "        else:\n",
    "            flows_csii = cs_flows.loc[cs_flows.scenario==scenario_climate,row.DV].sum(axis=1)\n",
    "        print(flows_csii.sum())\n",
    "        pf_links_mod.loc[pf_links_mod.edge==link,'lower_bound'] = flows_csii.values\n",
    "        pf_links_mod.loc[pf_links_mod.edge==link,'upper_bound'] = flows_csii.values\n",
    "        pf_flows_compare = pf_flows_compare.append(pd.DataFrame({\n",
    "            'date':flows_original.date,\n",
    "            'year':flows_original.year,\n",
    "            'month':flows_original.month,\n",
    "            'link':link,\n",
    "            'calsim':flows_csii,\n",
    "            'calvin':flows_original.lower_bound\n",
    "        }))\n",
    "# %% \n",
    "pf_links_mod_out = pf_links_mod[['i','j','k','cost','amplitude','lower_bound','upper_bound']]\n",
    "pf_links_mod_out.to_csv(os.path.join(pf_dir,'links82yr.csv'.format(scenario_name)),index=False)\n",
    "\n",
    "\n",
    "\n",
    "#### INFLOWS FOR LF ###\n",
    "# %% Extract monthly inflows to csv\n",
    "# load inflows \n",
    "inflow_qwry = pf_links_mod.loc[(pf_links_mod.i.str.contains('INFLOW'))]\n",
    "# split j to node and date\n",
    "inflows = inflow_qwry['j'].str.split('.',expand=True)\n",
    "inflows.columns = ['j','date']\n",
    "inflows['date'] = pd.DatetimeIndex(inflows['date'])\n",
    "inflows.set_index('date',inplace=True)\n",
    "# get inflow values\n",
    "inflows.insert(1,'flow_taf', value = inflow_qwry['lower_bound'].values)\n",
    "\n",
    "# %%  save out inflows output\n",
    "inflows.to_csv(os.path.join(lf_dir,'inflows.csv'))\n",
    "\n",
    "\n",
    "\n",
    "### INITIAL STORAGE ON LF LINKS ###\n",
    "# %% Init storages \n",
    "links = pd.read_csv('links.csv')\n",
    "# %% set initial storages on LF links\n",
    "for sr in cs_sr_init.sr:\n",
    "    links.loc[(links.i=='INITIAL') & (links.j.str.contains(sr)), ['lower_bound','upper_bound']] = cs_sr_init.loc[cs_sr_init.sr==sr,'calsim'].iloc[0]\n",
    "# %%\n",
    "links.to_csv(os.path.join(lf_dir,'links.csv'),index=False)\n",
    "\n",
    "\n",
    "### REVISE R_DICT ####\n",
    "# %%\n",
    "with open('r-dict.json') as f: \n",
    "    R_DICT = json.load(f)\n",
    "for sr in cs_sr_init.sr:\n",
    "    R_DICT[sr]['eop_init'] = float(cs_sr_init.loc[cs_sr_init.sr==sr,'calsim'].iloc[0])\n",
    "# %% save out the reservoir dictionary to json file\n",
    "R_DICT_out = R_DICT\n",
    "with open(os.path.join(lf_dir,'r-dict.json'), 'w') as json_file:\n",
    "    json.dump(R_DICT_out, json_file, \n",
    "        sort_keys=False, indent=4, separators=(',', ': '))\n",
    "\n",
    "\n",
    "#################################\n",
    "#################################\n",
    "#################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### FIXED STORAGE ON LF LINKS ###\n",
    "# %% set scenario name to fixed storage (FS)\n",
    "scenario_name = 'fs'.format(scenario_climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eoy_storages = pd.DataFrame()\n",
    "for idx,row in cs_map.iterrows():\n",
    "    if row.S:\n",
    "        link = str(row.SR)\n",
    "        print(link)\n",
    "        eoy_storage = cs_flows.loc[cs_flows.scenario==scenario_climate, ['datesim',row.S[0]]][11::12]\n",
    "        eoy_storage.columns = ['date','lower_bound']\n",
    "        eoy_storage.insert(2,'upper_bound',value=eoy_storage.lower_bound)\n",
    "        eoy_storage.insert(1,'i',value='{}.'.format(link)+eoy_storage['date'].astype(str))\n",
    "        eoy_storage.insert(2,'j',value='FINAL')\n",
    "        eoy_storage.insert(3,'k',value=0)\n",
    "        eoy_storages = eoy_storages.append(eoy_storage)\n",
    "# %% load variable constraints and append final storage contraints\n",
    "variable_constraints = pd.read_csv('variable-constraints.csv')\n",
    "variable_constraints = variable_constraints.append(eoy_storages)\n",
    "# %%\n",
    "lf_dir = '../../../my-models/calvin-lf-{}'.format(scenario_name)\n",
    "variable_constraints.to_csv(os.path.join(lf_dir,'variable-constraints.csv'),index=False)\n",
    "links.to_csv(os.path.join(lf_dir,'links.csv'),index=False)\n",
    "inflows.to_csv(os.path.join(lf_dir,'inflows.csv'))\n",
    "\n",
    "\n",
    "### REVISE RESERVOIR DICTIONARY ###\n",
    "# %% Reservoirs identified as COSVF canditates (Type 1)\n",
    "r_list = pf_links_mod.loc[(pf_links_mod.i_node.str.startswith('INITIAL'))].j_node.unique()\n",
    "r_type1 = ['SR_BER','SR_BUL','SR_CLK_INV','SR_CMN',\n",
    "            'SR_EBMUD','SR_HTH','SR_ISB','SR_LL_ENR','SR_LVQ',\n",
    "            'SR_PAR','SR_PNF','SR_RLL_CMB',\n",
    "            'SR_SFAGG','SR_GNT']\n",
    "r_type2 = ['GW_01', 'GW_02', 'GW_03','GW_04', 'GW_05', 'GW_06','GW_07',\n",
    "           'GW_08', 'GW_09', 'GW_10', 'GW_11', 'GW_12', 'GW_13', 'GW_14', 'GW_15',\n",
    "           'GW_16', 'GW_17', 'GW_18', 'GW_19', 'GW_20', 'GW_21',\n",
    "           'GW_AV', 'GW_CH', 'GW_EW', 'GW_IM', 'GW_MJ', 'GW_MWD',\n",
    "           'GW_OW', 'GW_SBV', 'GW_SC', 'GW_SD', 'GW_VC']\n",
    "# reservoir dictionary for calvin limited foresight run\n",
    "r_dict = dict()\n",
    "i = 0\n",
    "for r in r_list:\n",
    "    # initial storage value\n",
    "    initial_storage = pf_links_mod.loc[\n",
    "        (pf_links_mod.i_node=='INITIAL') & (pf_links_mod.j_node==r)].lower_bound\n",
    "    # lower bound on carryover\n",
    "    lb_9 = pf_links_mod.loc[\n",
    "        (pf_links_mod.i_node==r) & (pf_links_mod.j_node==r) & \n",
    "        (pf_links_mod.k==0) & (pf_links_mod.month==9)].lower_bound.min()\n",
    "    # upper bound on carryover from first year\n",
    "    ub_9 = pf_links_mod.loc[\n",
    "        (pf_links_mod.i_node==r) & (pf_links_mod.j_node==r) & \n",
    "        (pf_links_mod.month==9) & (pf_links_mod.year==1922)].upper_bound.sum()\n",
    "    # check COSVF Type 1 to index COSVF param\n",
    "    if r in r_type1:\n",
    "        r_type, cosvf_param_index, k_count, i = 1, [i,i+1], 15, i+2\n",
    "    elif r in r_type2:\n",
    "        r_type, cosvf_param_index, k_count, i = 2, i, 2, i+1\n",
    "    else:\n",
    "        r_type, cosvf_param_index, k_count = 0, None, 1\n",
    "    # add to reservoir dictionary\n",
    "    r_dict[r] = dict([\n",
    "        ('eop_init',initial_storage.values[0]),\n",
    "        ('lb',lb_9),\n",
    "        ('ub',ub_9),\n",
    "        ('type',r_type), \n",
    "        ('cosvf_param_index',cosvf_param_index),\n",
    "        ('k_count',k_count)])\n",
    "# %% save out the reservoir dictionary to json file\n",
    "lf_dir = '../../../my-models/calvin-lf-{}'.format(scenario_name)\n",
    "with open(os.path.join(lf_dir,'r-dict.json'), 'w') as json_file:\n",
    "    json.dump(r_dict, json_file, \n",
    "        sort_keys=False, indent=4, separators=(',', ': '))\n",
    "\n",
    "\n",
    "###  REVISE COSVF BASED ON RTYPES IN USE FOR FIXED STORAGE ###\n",
    "# %% Create default COSVF params\n",
    "param=['pmin','pmax']\n",
    "rtype1_list = list({key: value for key, value in r_dict.items() if value['type'] == 1}.keys())\n",
    "rtype2_list = list({key: value for key, value in r_dict.items() if value['type'] == 2}.keys())\n",
    "pos_r_list = rtype2_list + list(np.repeat(rtype1_list, len(param)))\n",
    "cosvf_pminmax = pd.DataFrame({'value':\n",
    "    list(np.repeat([-1e-6], len(rtype2_list))) + list(np.tile([-2e-2,-2e-2], len(rtype1_list)))})\n",
    "cosvf_pminmax.insert(0,'r',value=pos_r_list)\n",
    "cosvf_pminmax.insert(1,'param',value=list(['p'] * len(rtype2_list) + param * len(rtype1_list)))\n",
    "# %% save out default COSVF params\n",
    "lf_dir = '../../../my-models/calvin-lf-{}'.format(scenario_name)\n",
    "cosvf_pminmax.to_csv(os.path.join(lf_dir,'cosvf-params-.csv'),index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
